{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2 Deep QLearning\n",
    "\n",
    "Dans ce TP, vous devez implémenter un agent apprenant à faire atterir un vaisseau sur la lune avec l'algorithme Deep Q-Network. Pour cela vous allez utiliser [PyTorch](https://pytorch.org/) et [Gymnasium](https://gymnasium.farama.org/). \n",
    "\n",
    "<img src='img/lunarlander.png'  width=500px>\n",
    "\n",
    "# 1. Consignes\n",
    "\n",
    "> Vous devez  <span style=\"color:red\">compléter ce notebook et les différents fichiers python associés</span>.  <span style=\"color:red\"> Vous commenterez votre code</span> dans le notebook et dans les fichiers python.\n",
    "> \n",
    "> Vous devez aussi rendre un <span style=\"color:red\">rapport en pdf</span> (à ajouter à votre dépôt) avec le détail des différentes expérimentations réalisées dans les parties 6 et 7, i.e. pour chaque apprentissage testé, préciser dans le rapport les différents hyper paramètres (architecture du réseau, coefficient(s) d'apprentissage, nombre d'épisodes, taille du buffer, décroissance d'epsilon, ... tout ce qui permet de reproduire l'expérience) et mettre la courbe d'apprentissage obtenu.\n",
    "\n",
    "> Le code doit être fonctionnel avec l'environnement virtuel du TP. Si d'autres packages que ceux présents dans l'environnement virtuel créé au départ sont nécessaires, vous devez ajouter à votre dépôt un fichier `environnement.yaml` qui est un export de votre environnement virtuel. Ce fichier est obtenu avec la commande suivante:  ```conda env export > environnement.yaml```\n",
    "\n",
    "\n",
    "\n",
    "# 2. Import des packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gymnasium\n",
    "\n",
    "En apprentissage par renforcement, il y a deux concepts fondamentaux : l’agent et l’environnement.\n",
    "- L’agent est l’entité apprenante qui observe l’environnement et agit sur celui-ci selon les actions disponibles. Son objectif est de maximiser la récompense cumulée qu’il recoit de l’environnement avec lequel il interagit.\n",
    "- L'agent interagit avec l'environnement à travers la boucle de perception/action ce qui nécessite de définir :\n",
    "    - Un espace d’action.\n",
    "    - Un espace d’état (ou observation).\n",
    "    - Une fonction de récompense.\n",
    "  \n",
    "[Gymnasium](https://gymnasium.farama.org/) propose une interface open source unifiée entre un agent et un environnement.\n",
    "- [Gymnasium](https://gymnasium.farama.org/) propose un ensemble d'environnements pour des tâches d'apprentissage par renforcement. La plupart des environnements ont leur code source disponible sur [GitHub](https://github.com/Farama-Foundation/Gymnasium/tree/main/gymnasium/envs). De nouveaux environnements peuvent aussi être créés à condition qu'ils soient compatibles avec l'interface. \n",
    "- Grâce à l'interface unifiée, il est possible de définir indépendamment un agent de l’environnement avec lequel il interagit (et inversement). \n",
    "- Lorsque certains pré-traitements sont nécessaires sur les actions, observations, récompenses, ... il est possible d’encapsuler l’environnement dans un **wrapper**, celui-ci se chargera du pré-traitement. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce TP, nous allons implémenter un agent qui interagira avec l'environnement [LunarLander](https://gymnasium.farama.org/environments/box2d/lunar_lander/). Il existe plusieurs fonctions clé pour interagir avec un environnement.\n",
    "\n",
    ">  <span style=\"color:green\">Lisez la documentation de Gymnasium</span>: [utilisation basique](https://gymnasium.farama.org/content/basic_usage/), [API pour les environnements](https://gymnasium.farama.org/api/env/), ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3.1 - Caractéristiques de l'environnement LunarLander-v2\n",
    "\n",
    "> <span style=\"color:green\">Compléter la cellule de code ci-dessous. A l'exécution devra s'afficher:\n",
    "> \n",
    "> - **les dimensions pour les espaces d'états et d'actions** de l'environnement `LunarLander`.\n",
    "> - les bornes min et max pour les dimensions de l'état\n",
    "> - un échantillon pris au hasard dans chaque espace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Interaction et affichage de l'environnement sur un épisode\n",
    "\n",
    "> <span style=\"color:green\">Exécuter une instance de l'environnement `LunarLander` pendant un épisode avec des **actions aléatoires**. Afficher l'environnement à chaque pas pour visualiser le comportement du vaisseau. A la fin de l'épisode, afficher la somme des récompenses obtenues sur l'épisode et la raison pour laquelle l'épisode s'est terminé. </span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Agent glouton (sans apprentissage)\n",
    "\n",
    "> <span style=\"color:green\">Vous devez maintenant implémenter un **agent glouton** qui utilise un réseau de neurones pour représenter la Q fonction.</span>\n",
    "\n",
    "- définir l'architecture du réseau de neurones en complétant `QNN.py`. Ce réseau va approximer la Q-fonction comme dans DQN (pour l'instant, les poids du réseau ne seront pas mis à jour, le réseau est uniquement utilisé en prédiction).\n",
    "\n",
    "- définir un agent glouton en complétant `agentglouton.py`: il utilisera la prédiction du réseau de neurones pour choisir ses actions selon une stratégie d’exploration $\\epsilon$-greedy.\n",
    "\n",
    "- utiliser cet agent sur plusieurs épisodes dans `LunarLander`. Vous utiliserez une décroissance de l'exploration, i.e. que $\\epsilon$ va décroitre à chaque épisode, en démarrant à une valeur élevée (beaucoup d'exploration) et avec une borne minimum. Ainsi, au premier épisode, $\\epsilon=\\epsilon_{start}$, et à chaque épisode, $\\epsilon=max(\\epsilon_{end}, \\epsilon_{decay}*\\epsilon)$. Par exemple sur 1000 épisodes, les valeurs peuvent être $\\epsilon_{start} = 1.0$, $\\epsilon_{end} = 0.01$ et $\\epsilon_{decay} = 0.995$.\n",
    "\n",
    "- proposer un tracé de la somme des récompenses obtenues par épisode (vous pouvez utiliser le fichier `utils.py`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload all modules every time before executing the Python code typed\n",
    "%autoreload 2\n",
    "# import depuis un fichier python local \n",
    "from QNN import QNN \n",
    "from agentglouton import AgentGlouton\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Experience replay\n",
    "\n",
    "L'algorithme DQN utilise l’experience replay:\n",
    "- pendant la phase de **sample**, qui est une phase d'interaction avec l'environnement sans apprentissage, l'agent stocke en mémoire toutes les interactions (ou expériences) rencontrées. Une interaction est un tuple `(état,action,état_suivant,récompense,fin_episode)`.  Le buffer a une taille maximale (100 000 par exemple). Lorsqu’elle est dépassée, les nouvelles interactions remplacent les plus anciennes. \n",
    "\n",
    "- pendant la phase d'**apprentissage**, l’agent apprend à partir des interactions stockées dans son buffer. Il choisit aléatoirement un minibatch d’interactions dans son buffer (64 par exemple).\n",
    "\n",
    "> <span style=\"color:green\">Compléter la classe `ReplayBuffer` du fichier `replaybuffer.py` pour stockez les interactions et récupérer des mini-batchs. </span> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from replaybuffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Deep QLearning avec Replay Buffer\n",
    "\n",
    "\n",
    "Vous allez maintenant implémenter l'algorithme du **Deep QLearning avec ReplayBuffer** pour que votre agent apprenne en mettant à jour les poids de son réseau de neurone. \n",
    "\n",
    "- Après chaque interaction, le buffer sera remplit. \n",
    "- Une phase d'apprentissage sera réalisée toutes les `n` interactions (par ex. $n=4$), pendant laquelle un minibatch de données sera choisi dans le buffer pour réaliser la descente de gradient. Pour une interaction $(s,a,s',r)$, la valeur cible (erreur TD) sera calculée de la façon suivante:\n",
    "    - $y = r(s,a) + \\gamma max_b Q_\\omega(s',b) $ si l'épisode continu\n",
    "    - $y = r(s,a)  $ si l'épisode se termine\n",
    "\n",
    "Remarque: vous n'utiliserez pas de réseau cible (*target network*) pour l'instant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:green\">Compléter la classe `AgentDQN` et utiliser cet agent sur plusieurs épisodes dans `LunarLander`. Vous proposerez un tracé de la somme des récompenses obtenues par épisode (vous pouvez utiliser `utils.py`), et une visualisation d'un épisode après apprentissage.</span> \n",
    " \n",
    "Pour l'optimizer, SGD et [Adam](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/) sont particulièrement adaptés. \n",
    "\n",
    "Voici aussi des liens vers différentes fonctions de PyTorch qui pourraient vous être utiles:\n",
    "- [unsqueeze](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html)\n",
    "- [gather](https://pytorch.org/docs/stable/generated/torch.gather.html)\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%autoreload` not found.\n"
     ]
    }
   ],
   "source": [
    "#Reload all modules every time before executing the Python code typed\n",
    "%autoreload 2\n",
    "\n",
    "from agentdqn import AgentDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Deep QLearning avec réseau cible\n",
    "\n",
    "Il se peut que votre agent précédent apprenne des comportements intéressants mais qu’ils soient très instables. On va maintenant ajouter un réseau cible pour l'améliorer.\n",
    "\n",
    "> <span style=\"color:green\"> Compléter la classe `AgentDQNTarget` pour implémenter un agent apprenant avec DQN (deep QLearning et *target network*) pour stabiliser l’apprentissage. Utiliser cet agent sur plusieurs épisodes dans `LunarLander`. Vous proposerez un tracé de la somme des récompenses obtenues par épisode (vous pouvez utiliser `utils.py`), et une visualisation d'un épisode après apprentissage.</span> \n",
    " \n",
    " \n",
    "Pour la mise à jour du réseau cible, vous pouvez:\n",
    "- toutes les N étapes d'apprentissage (10000 par exemple), recopier entièrement le réseau de neurone original dans le duplicat\n",
    "- mettre à jour petit à petit le duplicat à chaque étape d’apprentissage : $$\\omega' = (1-\\tau)\\omega' +\\alpha \\omega$$ où $\\omega'$ sont les poids du duplicat, $\\omega$ les poids du réseau original, et $\\tau$ le pas de mise à jour. Souvent $\\tau = 0.001$.\n",
    "\n",
    "\n",
    "Pour copier des poids d'un réseau de neurone vers un autre, la méthode `copy_` peut être appelée sur les paramètres:\n",
    "\n",
    "`for param_duplicat, param_source in zip(model_duplicat.parameters(), model_source.parameters()):`\n",
    "           \n",
    "`param_duplicat.data.copy_(param_source.data)`\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentdqntarget import AgentDQNTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sauvegarde reseau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sauvegarde du réseau \n",
    "savedfile = 'checkpoint_DQNtarget_3232.pth'\n",
    "torch.save(dqlagent.qnetwork_local.state_dict(), savedfile)\n",
    "#chargement d'un reseau sauvegarde\n",
    "state_dict = torch.load(savedfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
